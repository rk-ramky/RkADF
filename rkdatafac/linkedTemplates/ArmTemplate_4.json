{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "rkdatafac"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/df_Parse')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_SQ",
								"type": "DatasetReference"
							},
							"name": "sourcetblemployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkemployees"
						}
					],
					"transformations": [
						{
							"name": "parseskills"
						},
						{
							"name": "parseaddress"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as integer,",
						"          empName as string,",
						"          skills as string,",
						"          address as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> sourcetblemployees",
						"sourcetblemployees parse(Parseskills = skills ? (skill1 as string,",
						"          skill2 as string,",
						"          skill3 as string),",
						"     format: 'delimited',",
						"     columnNamesAsHeader: false,",
						"     columnDelimiter: '|',",
						"     nullValue: '') ~> parseskills",
						"parseskills parse(parsedjson = address ? (city as string,",
						"          country as string),",
						"     format: 'json',",
						"     documentForm: 'singleDocument') ~> parseaddress",
						"parseaddress sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['empparse.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empId,",
						"          empName,",
						"          skill1 = Parseskills.skill1,",
						"          skill2 = Parseskills.skill2,",
						"          skill3 = Parseskills.skill3,",
						"          city = parsedjson.city,",
						"          country = parsedjson.country",
						"     ),",
						"     partitionBy('hash', 1)) ~> sinkemployees"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Pivot')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "sourceEmpNew"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkPivot"
						}
					],
					"transformations": [
						{
							"name": "pivotTrans"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          email as string,",
						"          location as string,",
						"          department as string,",
						"          Gender as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceEmpNew",
						"sourceEmpNew pivot(groupBy(department),",
						"     pivotBy(Gender),",
						"     {} = count(id),",
						"     columnNaming: 'Total_$N$V_Employees',",
						"     lateral: true) ~> pivotTrans",
						"pivotTrans sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['EmpPivot.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkPivot"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Rank')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinksal"
						}
					],
					"transformations": [
						{
							"name": "ranksal"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as integer,",
						"          ename as string,",
						"          job as string,",
						"          mgr as integer,",
						"          hiredate as date,",
						"          sal as integer,",
						"          comm as integer,",
						"          deptno as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     preferredIntegralType: 'integer') ~> sourceemp",
						"sourceemp rank(desc(sal, true),",
						"     output(SalaryRank as long),",
						"     dense: true) ~> ranksal",
						"ranksal sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['Emp_Rank.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinksal"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_RomoveDup')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "sourceEmployee"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_Source2",
								"type": "DatasetReference"
							},
							"name": "sourceEmployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sinkunion"
						}
					],
					"transformations": [
						{
							"name": "unionEmps"
						},
						{
							"name": "aggdeleteduplicates"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          department as string,",
						"          location as string,",
						"          Gender as string,",
						"          salary as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceEmployee",
						"source(output(",
						"          Name as string,",
						"          department as string,",
						"          location as string,",
						"          Gender as string,",
						"          salary as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceEmployees",
						"sourceEmployee, sourceEmployees union(byName: true)~> unionEmps",
						"unionEmps aggregate(groupBy(Name),",
						"     each(match(name!='Name'), $$ = first($$))) ~> aggdeleteduplicates",
						"aggdeleteduplicates sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['emporiginals.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkunion"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_RunTot')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "Order"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "ordertottgt"
						}
					],
					"transformations": [
						{
							"name": "RunTot"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PO as string,",
						"          itemname as string,",
						"          quantity as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Order",
						"Order window(asc(PO, true),",
						"     RunningTotal = sum(toInteger(quantity))) ~> RunTot",
						"RunTot sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['orderruntot.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> ordertottgt"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_SCD1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "Employ"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_SQL",
								"type": "DatasetReference"
							},
							"name": "tblEmploy"
						}
					],
					"transformations": [
						{
							"name": "alterRow"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EmpId as integer,",
						"          Name as string,",
						"          Country as string,",
						"          Department as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employ",
						"Employ alterRow(upsertIf(1==1)) ~> alterRow",
						"alterRow sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          EmpId as integer,",
						"          Name as string,",
						"          Country as string,",
						"          Department as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:false,",
						"     upsertable:true,",
						"     keys:['EmpId'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          EmpId,",
						"          Name,",
						"          Country,",
						"          Department",
						"     )) ~> tblEmploy"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Select')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkselect"
						}
					],
					"transformations": [
						{
							"name": "selectemp"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as integer,",
						"          ename as string,",
						"          job as string,",
						"          mgr as integer,",
						"          hiredate as date,",
						"          sal as integer,",
						"          comm as integer,",
						"          deptno as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceemp",
						"sourceemp select(mapColumn(",
						"          empNumber = empno,",
						"          deptno,",
						"          Ename = ename,",
						"          salary = sal",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectemp",
						"selectemp sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['empselect.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkselect"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Stringfy')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Source_Json",
								"type": "DatasetReference"
							},
							"name": "sourcejkjson"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkjkjson"
						}
					],
					"transformations": [
						{
							"name": "stringifyjkjson"
						},
						{
							"name": "derivedjkjson"
						}
					],
					"scriptLines": [
						"source(output(",
						"          name as string,",
						"          skills as string[],",
						"          contact as (mobile as string, landline as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> sourcejkjson",
						"sourcejkjson stringify(sf_contact = contact ? string,",
						"     format: 'json') ~> stringifyjkjson",
						"stringifyjkjson derive(sf_contact = toString(sf_contact)) ~> derivedjkjson",
						"derivedjkjson sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['jk_stringfy.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          name,",
						"          contact = sf_contact",
						"     ),",
						"     partitionBy('hash', 1)) ~> sinkjkjson"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Type2')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "srcEmploy"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_SQ",
								"type": "DatasetReference"
							},
							"name": "tgtEmploytype2"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_SQ",
								"type": "DatasetReference"
							},
							"name": "EmploySink"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_SQ",
								"type": "DatasetReference"
							},
							"name": "UpdateSink"
						}
					],
					"transformations": [
						{
							"name": "dcAddColumnIsActive"
						},
						{
							"name": "selectRenameColumns"
						},
						{
							"name": "lkpSrcSink"
						},
						{
							"name": "filExistingEmploys"
						},
						{
							"name": "seltoremoveunwantedcolumns"
						},
						{
							"name": "alterRow"
						},
						{
							"name": "dcupdateisActive"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EmpId as integer,",
						"          Name as string,",
						"          Country as string,",
						"          Department as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> srcEmploy",
						"source(output(",
						"          surrKey as integer,",
						"          empid as integer,",
						"          Name as string,",
						"          Country as string,",
						"          Department as integer,",
						"          isActive as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> tgtEmploytype2",
						"srcEmploy derive(isActive = 1) ~> dcAddColumnIsActive",
						"tgtEmploytype2 select(mapColumn(",
						"          SQL_surrKey = surrKey,",
						"          SQL_empid = empid,",
						"          SQL_Name = Name,",
						"          SQL_Country = Country,",
						"          SQL_Department = Department,",
						"          SQL_isActive = isActive",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectRenameColumns",
						"srcEmploy, selectRenameColumns lookup(EmpId == SQL_empid,",
						"     multiple: true,",
						"     broadcast: 'auto')~> lkpSrcSink",
						"lkpSrcSink filter(!isNull(SQL_empid)) ~> filExistingEmploys",
						"filExistingEmploys select(mapColumn(",
						"          SQL_surrKey,",
						"          SQL_empid,",
						"          SQL_Name,",
						"          SQL_Country,",
						"          SQL_Department,",
						"          SQL_isActive",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> seltoremoveunwantedcolumns",
						"dcupdateisActive alterRow(updateIf(1==1)) ~> alterRow",
						"seltoremoveunwantedcolumns derive(SQL_isActive = 0) ~> dcupdateisActive",
						"dcAddColumnIsActive sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          surrKey as integer,",
						"          empid as integer,",
						"          Name as string,",
						"          Country as string,",
						"          Department as integer,",
						"          isActive as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          empid = EmpId,",
						"          Name,",
						"          Country,",
						"          Department,",
						"          isActive",
						"     )) ~> EmploySink",
						"alterRow sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          surrKey as integer,",
						"          empid as integer,",
						"          Name as string,",
						"          Country as string,",
						"          Department as integer,",
						"          isActive as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['surrKey'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 1,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          surrKey = SQL_surrKey,",
						"          empid = SQL_empid,",
						"          Name = SQL_Name,",
						"          Country = SQL_Country,",
						"          Department = SQL_Department,",
						"          isActive = SQL_isActive",
						"     )) ~> UpdateSink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_UDFunctions')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sourceEmployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumnudf"
						}
					],
					"udfLibraries": [
						{
							"referenceName": "UDFunctions",
							"type": "DataFlowReference"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          department as integer,",
						"          location as string,",
						"          Gender as string,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     preferredIntegralType: 'integer') ~> sourceEmployees",
						"sourceEmployees derive(Gender = GenderValue(Gender)) ~> derivedColumnudf",
						"derivedColumnudf sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['empudf.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Union')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "sourceempdata1"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_Source2",
								"type": "DatasetReference"
							},
							"name": "sourceempdata2"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sourceempdata3"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkempdata"
						}
					],
					"transformations": [
						{
							"name": "unionempdata"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          email as string,",
						"          location as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceempdata1",
						"source(output(",
						"          id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          email as string,",
						"          location as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceempdata2",
						"source(output(",
						"          id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          email as string,",
						"          location as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceempdata3",
						"sourceempdata1, sourceempdata2, sourceempdata3 union(byName: true)~> unionempdata",
						"unionempdata sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['emadatafull.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkempdata"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Var')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "sourceempinfo"
						}
					],
					"sinks": [
						{
							"name": "sinkmaxsal"
						}
					],
					"transformations": [
						{
							"name": "aggregatesal"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as integer,",
						"          Name as string,",
						"          salary as integer,",
						"          key as integer,",
						"          department as integer,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceempinfo",
						"sourceempinfo aggregate(MaxSalary = max(salary)) ~> aggregatesal",
						"aggregatesal sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1) ~> sinkmaxsal"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_WindowTran')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "sourcewindowtran"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkwindow"
						}
					],
					"transformations": [
						{
							"name": "windowemp"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as integer,",
						"          ename as string,",
						"          job as string,",
						"          mgr as integer,",
						"          hiredate as date,",
						"          sal as integer,",
						"          comm as integer,",
						"          deptno as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcewindowtran",
						"sourcewindowtran window(over(deptno),",
						"     desc(sal, true),",
						"     DenseRank = denseRank()) ~> windowemp",
						"windowemp sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['emp_window.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkwindow"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_assert')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Dept",
								"type": "DatasetReference"
							},
							"name": "sourcedept"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkempdept"
						}
					],
					"transformations": [
						{
							"name": "assertemp"
						},
						{
							"name": "derivedColumnerror"
						},
						{
							"name": "filterunwanted"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as integer,",
						"          ename as string,",
						"          hiredate as string,",
						"          sal as integer,",
						"          deptno as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceemp",
						"source(output(",
						"          deptno as integer,",
						"          dname as string,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcedept",
						"sourceemp, sourcedept assert(expectTrue(!isNull(toDate(hiredate, 'yyyyMMdd')), false, 'assertdoj', null, 'checking date of join'),",
						"     expectUnique(empno, false, 'assertempid', null, 'valid emp id'),",
						"     expectExists(sourceemp@deptno == sourcedept@deptno, false, 'assertdeptno', null, 'valid from dept table')) ~> assertemp",
						"assertemp derive(IsErrorRow = isError(),",
						"          IsInCorrectDeptrow = hasError('assertdeptno')) ~> derivedColumnerror",
						"derivedColumnerror filter(IsErrorRow == false()) ~> filterunwanted",
						"filterunwanted sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['empassert.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkempdept"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_derivedcolumns')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "empsource"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkderived"
						}
					],
					"transformations": [
						{
							"name": "derivedColumnlocation"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as string,",
						"          ename as string,",
						"          job as string,",
						"          mgr as string,",
						"          hiredate as string,",
						"          sal as string,",
						"          comm as string,",
						"          deptno as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> empsource",
						"empsource derive(jobName = initCap(mgr),",
						"          NewHiredate = iif(isNull(hiredate),'Unknown',hiredate)) ~> derivedColumnlocation",
						"derivedColumnlocation sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['empDerived.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkderived"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_error_rows')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "SalesInd"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_SQL",
								"type": "DatasetReference"
							},
							"name": "SalesBad"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_SQ",
								"type": "DatasetReference"
							},
							"name": "SalesGood"
						}
					],
					"transformations": [
						{
							"name": "BadRecs"
						},
						{
							"name": "ForExtraColumn"
						},
						{
							"name": "forExtraColumnGood"
						}
					],
					"scriptLines": [
						"source(output(",
						"          saleDate as string,",
						"          salesitem as string,",
						"          country as string,",
						"          qunatity as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     preferredIntegralType: 'integer') ~> SalesInd",
						"SalesInd split(isNull(toDate(saleDate,'dd-MMM-yyyy')),",
						"     disjoint: false) ~> BadRecs@(ErrorRecs, GoodRecs)",
						"BadRecs@ErrorRecs derive(fileName = 'RK') ~> ForExtraColumn",
						"BadRecs@GoodRecs derive(FileName = 'RK',",
						"          saleDate = toDate(saleDate, 'dd-MMM-yyyy'),",
						"          qunatity = toInteger(qunatity)) ~> forExtraColumnGood",
						"ForExtraColumn sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          EmpId as integer,",
						"          Name as string,",
						"          Country as string,",
						"          Department as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          saleDate,",
						"          salesitem,",
						"          country,",
						"          qunatity,",
						"          FILENAME = fileName",
						"     )) ~> SalesBad",
						"forExtraColumnGood sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          surrKey as integer,",
						"          empid as integer,",
						"          Name as string,",
						"          Country as string,",
						"          Department as integer,",
						"          isActive as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          saleDate,",
						"          salesitem,",
						"          country,",
						"          qunatity,",
						"          FILENAME = FileName",
						"     )) ~> SalesGood"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_filter_trans')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "Emp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "Emp30"
						}
					],
					"transformations": [
						{
							"name": "filteremp"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as string,",
						"          ename as string,",
						"          job as string,",
						"          mgr as string,",
						"          hiredate as string,",
						"          sal as string,",
						"          comm as string,",
						"          deptno as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Emp",
						"Emp filter(equals(deptno,'30')) ~> filteremp",
						"filteremp sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['Emp30.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> Emp30"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_fuzzyJoin')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Blob",
								"type": "DatasetReference"
							},
							"name": "Sales"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_Blb",
								"type": "DatasetReference"
							},
							"name": "SalesNew"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "joinfuzzy"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          Item as string,",
						"          Quantity as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Sales",
						"source(output(",
						"          Name as string,",
						"          Item as string,",
						"          Quantity as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> SalesNew",
						"Sales, SalesNew join(fuzzyCompare(Sales@Name, SalesNew@Name, 82.00),",
						"     joinType:'inner',",
						"     matchType:'fuzzy',",
						"     ignoreSpaces: false,",
						"     scoreColumn:'Similarity Level',",
						"     broadcast: 'off')~> joinfuzzy",
						"joinfuzzy sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['example.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_joinfilesflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "empsource"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "deptsource"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Dept",
								"type": "DatasetReference"
							},
							"name": "empdeptsink"
						}
					],
					"transformations": [
						{
							"name": "joinempdept"
						},
						{
							"name": "ModifyColumns1",
							"description": "Autogenerated by data preview actions"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as integer,",
						"          ename as string,",
						"          job as string,",
						"          mgr as integer,",
						"          hiredate as date,",
						"          sal as integer,",
						"          comm as integer,",
						"          deptno as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> empsource",
						"source(output(",
						"          deptno as integer,",
						"          dname as string,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> deptsource",
						"ModifyColumns1, deptsource join(empsource@deptno == deptsource@deptno,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinempdept",
						"empsource derive(job = lower(job)) ~> ModifyColumns1",
						"joinempdept sink(allowSchemaDrift: true,",
						"     validateSchema: true,",
						"     input(",
						"          deptno as string,",
						"          dname as string,",
						"          location as string",
						"     ),",
						"     partitionFileNames:['empdept.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empno,",
						"          ename,",
						"          sal,",
						"          deptno = empsource@deptno,",
						"          dname,",
						"          loc = location",
						"     ),",
						"     partitionBy('hash', 1),",
						"     format: 'table') ~> empdeptsink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_mapjoin')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_SQL",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_SQ",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SQL_Blob",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "join1",
							"description": "Inner join on dbo.emp and dbo.dept"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as integer,",
						"          ename as string,",
						"          job as string,",
						"          mgr as integer,",
						"          hiredate as date,",
						"          sal as integer,",
						"          comm as integer,",
						"          deptno as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> source1",
						"source(output(",
						"          deptno as integer,",
						"          dname as string,",
						"          loc as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     batchSize: 0,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> source2",
						"source1, source2 join(source1@deptno == source2@deptno,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> join1",
						"join1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		}
	]
}