{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "rkdatafac"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/pipeline1')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "LookupSp",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": "Declare @empTotal int\nexecute spGetEmpCountByGender 'Male', @empTotal output\nselect @empTotal as empTotal\n",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Azure_SQ",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-10-09T09:54:54Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Aggregate_PQ')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "WranglingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"name": "DS_Azure_CSV_Emp",
							"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> DS_Azure_CSV_Emp",
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							}
						}
					],
					"script": "section Section1;\r\nshared DS_Azure_CSV_Emp = let AdfDoc = AzureStorage.BlobContents(\"https://rkstoragecontainer.blob.core.windows.net/src/Emp.csv\"),Csv = Csv.Document(AdfDoc, [Delimiter = \",\", Encoding = TextEncoding.Utf8, QuoteStyle = QuoteStyle.Csv]), PromotedHeaders = Table.PromoteHeaders(Csv, [PromoteAllScalars = true]) in  PromotedHeaders;\r\nshared UserQuery = let Source = #\"DS_Azure_CSV_Emp\",\r\n  #\"Changed column type\" = Table.TransformColumnTypes(Source, {{\"empno\", Int64.Type}, {\"ename\", type text}, {\"job\", type text}, {\"mgr\", Int64.Type}, {\"hiredate\", type text}, {\"sal\", Int64.Type}, {\"comm\", Int64.Type}, {\"deptno\", Int64.Type}}),\r\n  #\"Grouped rows\" = Table.Group(#\"Changed column type\", {\"deptno\"}, {{\"TotSal\", each List.Sum([sal]), type nullable number}}) in #\"Grouped rows\";\r\n",
					"documentLocale": "en-us"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Merge_PQ')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "WranglingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"name": "DS_Azure_CSV_Emp",
							"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> DS_Azure_CSV_Emp",
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							}
						},
						{
							"name": "DS_Azure_CSV_Dept",
							"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> DS_Azure_CSV_Dept",
							"dataset": {
								"referenceName": "DS_Azure_CSV_Dept",
								"type": "DatasetReference"
							}
						}
					],
					"script": "section Section1;\r\nshared DS_Azure_CSV_Emp = let AdfDoc = AzureStorage.BlobContents(\"https://rkstoragecontainer.blob.core.windows.net/src/Emp.csv\"),Csv = Csv.Document(AdfDoc, [Delimiter = \",\", Encoding = TextEncoding.Utf8, QuoteStyle = QuoteStyle.Csv]), PromotedHeaders = Table.PromoteHeaders(Csv, [PromoteAllScalars = true]) in  PromotedHeaders;\r\nshared DS_Azure_CSV_Dept = let AdfDoc = AzureStorage.BlobContents(\"https://rkstoragecontainer.blob.core.windows.net/src/Dept.csv\"),Csv = Csv.Document(AdfDoc, [Delimiter = \",\", Encoding = TextEncoding.Utf8, QuoteStyle = QuoteStyle.Csv]), PromotedHeaders = Table.PromoteHeaders(Csv, [PromoteAllScalars = true]) in  PromotedHeaders;\r\nshared UserQuery = let Source = #\"DS_Azure_CSV_Emp\",\r\n  #\"Merged queries\" = Table.NestedJoin(Source, {\"deptno\"}, DS_Azure_CSV_Dept, {\"deptno\"}, \"DS_Azure_CSV_Dept\", JoinKind.Inner),\r\n  #\"Expanded DS_Azure_CSV_Dept\" = Table.ExpandTableColumn(#\"Merged queries\", \"DS_Azure_CSV_Dept\", {\"deptno\", \"dname\", \"location\"}, {\"deptno.1\", \"dname\", \"location\"}),\r\n  #\"Removed columns\" = Table.RemoveColumns(#\"Expanded DS_Azure_CSV_Dept\", {\"deptno.1\"}) in #\"Removed columns\";\r\n",
					"documentLocale": "en-us"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Employee_CSV",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Rule_Sink",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "filterGender"
						}
					],
					"scriptLines": [
						"parameters{",
						"     FilterRule as string",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 filter(toBoolean(expr($FilterRule))) ~> filterGender",
						"filterGender sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['ruleemp.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_AlterRow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "sourcealterrow"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_SQ",
								"type": "DatasetReference"
							},
							"name": "sinkemp"
						}
					],
					"transformations": [
						{
							"name": "alterRowempdata"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as integer,",
						"          first_name as string,",
						"          last_name as string,",
						"          salary as integer,",
						"          location as string,",
						"          department as integer,",
						"          Gender as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcealterrow",
						"sourcealterrow alterRow(deleteIf(department==50),",
						"     upsertIf(department==20)) ~> alterRowempdata",
						"alterRowempdata sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          surrKey as integer,",
						"          empid as integer,",
						"          Name as string,",
						"          Country as string,",
						"          Department as integer,",
						"          isActive as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['id'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sinkemp"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_CDC')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_SQL",
								"type": "DatasetReference"
							},
							"name": "sourcecitizen"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Sqldb",
								"type": "DatasetReference"
							},
							"name": "sinkcitizens"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          id as integer,",
						"          name as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     enableNativeCdc: true,",
						"     netChanges: true,",
						"     skipInitialLoad: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> sourcecitizen",
						"sourcecitizen sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as integer,",
						"          name as string",
						"     ),",
						"     deletable:true,",
						"     insertable:true,",
						"     updateable:true,",
						"     upsertable:true,",
						"     keys:['id'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          id,",
						"          name",
						"     )) ~> sinkcitizens"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_CSVtoJSON')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "Order"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sinkOrder"
						}
					],
					"transformations": [
						{
							"name": "OrderComplex"
						},
						{
							"name": "aggregateoncustId"
						}
					],
					"scriptLines": [
						"source(output(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Order",
						"Order derive(Purchases = @(itemName=itemName,",
						"          quantity=quantity)) ~> OrderComplex",
						"OrderComplex aggregate(groupBy(custId,",
						"          CustName),",
						"     Purchases = collect(Purchases)) ~> aggregateoncustId",
						"aggregateoncustId sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Json.json'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          custId,",
						"          CustName,",
						"          Purchases",
						"     ),",
						"     partitionBy('hash', 1)) ~> sinkOrder"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Cache')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_SQ",
								"type": "DatasetReference"
							},
							"name": "sourceEmployee"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sourceEmployees"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_Sqldb",
								"type": "DatasetReference"
							},
							"name": "SrcLocation"
						}
					],
					"sinks": [
						{
							"name": "sinkmaxId"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_SQL",
								"type": "DatasetReference"
							},
							"name": "sinkEmp"
						},
						{
							"name": "sinkLoc"
						}
					],
					"transformations": [
						{
							"name": "skId"
						},
						{
							"name": "dcExp"
						}
					],
					"scriptLines": [
						"source(output(",
						"          maxId as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: 'select max(id) as maxId from employee',",
						"     format: 'query') ~> sourceEmployee",
						"source(output(",
						"          Name as string,",
						"          department as integer,",
						"          location as string,",
						"          Gender as string,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     preferredIntegralType: 'integer') ~> sourceEmployees",
						"source(output(",
						"          code as string,",
						"          Courntry as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> SrcLocation",
						"sourceEmployees keyGenerate(output(empId as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> skId",
						"skId derive(empId = empId + sinkmaxId#outputs()[1].maxId,",
						"          location = sinkLoc#lookup(location).Courntry) ~> dcExp",
						"sourceEmployee sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: false,",
						"     saveOrder: 1,",
						"     mapColumn(",
						"          maxId",
						"     )) ~> sinkmaxId",
						"dcExp sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          EmpId as integer,",
						"          Name as string,",
						"          Country as string,",
						"          Department as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          ID = empId,",
						"          Name,",
						"          department,",
						"          location,",
						"          Gender,",
						"          salary",
						"     )) ~> sinkEmp",
						"SrcLocation sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     keys:['code'],",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: false,",
						"     saveOrder: 1,",
						"     mapColumn(",
						"          code,",
						"          Courntry",
						"     )) ~> sinkLoc"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Cast')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sourceemployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkgood"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkbad"
						}
					],
					"transformations": [
						{
							"name": "castempin"
						},
						{
							"name": "splitdate"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as string,",
						"          ename as string,",
						"          hiredate as string,",
						"          sal as string,",
						"          deptno as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceemployees",
						"sourceemployees cast(output(",
						"          empno as integer,",
						"          hiredate as date 'dd/MM/yyyy',",
						"          sal as integer,",
						"          deptno as integer",
						"     ),",
						"     errors: true) ~> castempin",
						"castempin split(!isError(),",
						"     disjoint: false) ~> splitdate@(GoodRecords, BadRecords)",
						"splitdate@GoodRecords sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sinkgood",
						"splitdate@BadRecords sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sinkbad"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Conditional Split')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "Emp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkdept10"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkdept20"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkdept30"
						}
					],
					"transformations": [
						{
							"name": "splitondeptno"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as string,",
						"          ename as string,",
						"          job as string,",
						"          mgr as string,",
						"          hiredate as string,",
						"          sal as string,",
						"          comm as string,",
						"          deptno as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Emp",
						"Emp split(equals(deptno, '10'),",
						"     equals(deptno, '20'),",
						"     equals(deptno, '30'),",
						"     disjoint: false) ~> splitondeptno@(dept10, dept20, dept30, otherdepts)",
						"splitondeptno@dept10 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['dept10'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkdept10",
						"splitondeptno@dept20 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['dept20'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkdept20",
						"splitondeptno@dept30 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['dept30'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkdept30"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Exists')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Dept",
								"type": "DatasetReference"
							},
							"name": "sourcdept"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkempdept"
						}
					],
					"transformations": [
						{
							"name": "existsempdept"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as string,",
						"          ename as string,",
						"          job as string,",
						"          mgr as string,",
						"          hiredate as string,",
						"          sal as string,",
						"          comm as string,",
						"          deptno as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceemp",
						"source(output(",
						"          deptno as string,",
						"          dname as string,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcdept",
						"sourceemp, sourcdept exists(sourceemp@deptno == sourcdept@deptno,",
						"     negate:true,",
						"     broadcast: 'auto')~> existsempdept",
						"existsempdept sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['deptexists.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkempdept"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_FixedFile')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Fixed_File",
								"type": "DatasetReference"
							},
							"name": "EmpFixed"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "fixedsink"
						}
					],
					"transformations": [
						{
							"name": "derivedColumnforColumns"
						},
						{
							"name": "RemoveColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Column_1 as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> EmpFixed",
						"EmpFixed derive(empno = substring(Column_1, 1, 5),",
						"          ename = substring(Column_1, 6, 6),",
						"          job = substring(Column_1, 12, 9),",
						"          mgr = substring(Column_1, 22, 4),",
						"          hiredate = substring(Column_1, 25, 10),",
						"          sal = substring(Column_1, 35, 4),",
						"          comm = substring(Column_1, 39,4),",
						"          deptno = substring(Column_1, 43, 45)) ~> derivedColumnforColumns",
						"derivedColumnforColumns select(mapColumn(",
						"          empno,",
						"          ename,",
						"          job,",
						"          mgr,",
						"          hiredate,",
						"          sal,",
						"          comm,",
						"          deptno",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> RemoveColumn1",
						"RemoveColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['emp_fixed.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> fixedsink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_FlattenTran')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Source_Json",
								"type": "DatasetReference"
							},
							"name": "sourcerk"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkflat"
						}
					],
					"transformations": [
						{
							"name": "flattenrkjson"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as string,",
						"          name as string,",
						"          skills as string[],",
						"          Address as (state as string, country as string, zipcode as string),",
						"          Contact as (Phone as string, email as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'singleDocument') ~> sourcerk",
						"sourcerk foldDown(unroll(skills),",
						"     mapColumn(",
						"          id,",
						"          name,",
						"          skills,",
						"          Address = Address.state,",
						"          {Address.zipcode} = Address.country,",
						"          Contact = Contact.Phone,",
						"          {Contact.Phone} = Contact.email",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flattenrkjson",
						"flattenrkjson sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['rk_json_flatten.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkflat"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Flowlet')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sourceempl"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkflowlet"
						}
					],
					"transformations": [
						{
							"name": "aggregatedup"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Id as integer,",
						"          Name as string,",
						"          department as integer,",
						"          location as string,",
						"          Gender as string,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceempl",
						"sourceempl aggregate(groupBy(Id,",
						"          Name,",
						"          department,",
						"          salary),",
						"     count = count()) ~> aggregatedup",
						"aggregatedup sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['empflowlet.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          Id,",
						"          Name,",
						"          department,",
						"          salary",
						"     ),",
						"     partitionBy('hash', 1)) ~> sinkflowlet"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Flowletdept')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source2",
								"type": "DatasetReference"
							},
							"name": "sourcedep"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SQL_Blob",
								"type": "DatasetReference"
							},
							"name": "sinkdep"
						}
					],
					"transformations": [
						{
							"name": "aggregatedep"
						}
					],
					"scriptLines": [
						"source(output(",
						"          deptno as integer,",
						"          dname as string,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcedep",
						"sourcedep aggregate(groupBy(deptno,",
						"          dname,",
						"          location),",
						"     count = count()) ~> aggregatedep",
						"aggregatedep sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sinkdep"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Incr')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "sourcetgt"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_Source2",
								"type": "DatasetReference"
							},
							"name": "sourcesrc"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sinkorg"
						}
					],
					"transformations": [
						{
							"name": "aggMaxId"
						},
						{
							"name": "dcdummy"
						},
						{
							"name": "joinsrctgt"
						},
						{
							"name": "skId"
						},
						{
							"name": "dcsum"
						},
						{
							"name": "selecttorcols"
						},
						{
							"name": "unionwithsrc"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Id as string,",
						"          Name as string,",
						"          department as string,",
						"          location as string,",
						"          Gender as string,",
						"          salary as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcetgt",
						"source(output(",
						"          Name as string,",
						"          department as string,",
						"          location as string,",
						"          Gender as string,",
						"          salary as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcesrc",
						"dcdummy aggregate(groupBy(dummy),",
						"     MaxId = max(Id)) ~> aggMaxId",
						"sourcetgt derive(dummy = 'dummy') ~> dcdummy",
						"aggMaxId, sourcesrc join(1==1,",
						"     joinType:'cross',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinsrctgt",
						"joinsrctgt keyGenerate(output(Id as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> skId",
						"skId derive(Id = toString(toInteger(Id) + toInteger(MaxId))) ~> dcsum",
						"dcsum select(mapColumn(",
						"          Id,",
						"          Name,",
						"          department,",
						"          location,",
						"          Gender,",
						"          salary",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selecttorcols",
						"sourcetgt, selecttorcols union(byName: true)~> unionwithsrc",
						"unionwithsrc sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['emporg.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkorg"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Lookup')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Dept",
								"type": "DatasetReference"
							},
							"name": "sourcedept"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinklookup"
						}
					],
					"transformations": [
						{
							"name": "lookupempdept"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as string,",
						"          ename as string,",
						"          job as string,",
						"          mgr as string,",
						"          hiredate as string,",
						"          sal as string,",
						"          comm as string,",
						"          deptno as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceemp",
						"source(output(",
						"          deptno as string,",
						"          dname as string,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcedept",
						"sourceemp, sourcedept lookup(sourceemp@deptno == sourcedept@deptno,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     broadcast: 'auto')~> lookupempdept",
						"lookupempdept sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['empdeptlookup.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinklookup"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_MergeCols')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "EmployesSource"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sinkempskills"
						}
					],
					"transformations": [
						{
							"name": "aggforGroupby"
						},
						{
							"name": "dcforseperatecolumns"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as string,",
						"          empname as string,",
						"          skills as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> EmployesSource",
						"EmployesSource aggregate(groupBy(empid,",
						"          empname),",
						"     skills = collect(skills)) ~> aggforGroupby",
						"aggforGroupby derive(skills = replace(replace(replace(toString(skills),'[',''),']',''),'\"','')) ~> dcforseperatecolumns",
						"dcforseperatecolumns sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['empSkills.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkempskills"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_NewBranch')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Dept",
								"type": "DatasetReference"
							},
							"name": "sourcedept"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "aggregateemp"
						},
						{
							"name": "joinempdept"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as integer,",
						"          ename as string,",
						"          job as string,",
						"          mgr as integer,",
						"          hiredate as string,",
						"          sal as integer,",
						"          comm as integer,",
						"          deptno as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceemp",
						"source(output(",
						"          deptno as integer,",
						"          dname as string,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcedept",
						"sourceemp aggregate(groupBy(deptno),",
						"     {Total Employees} = count(empno)) ~> aggregateemp",
						"sourceemp, sourcedept join(sourceemp@deptno == sourcedept@deptno,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinempdept",
						"joinempdept sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['empjoinout.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1",
						"aggregateemp sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['emptotal.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink2"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_ParameterizeMapping')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sourceempnew"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkemp"
						}
					],
					"transformations": [
						{
							"name": "filterempdata"
						}
					],
					"scriptLines": [
						"parameters{",
						"     Department as integer",
						"}",
						"source(output(",
						"          id as integer,",
						"          Name as string,",
						"          Gender as string,",
						"          salary as integer,",
						"          department as integer,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceempnew",
						"sourceempnew filter(department == $Department) ~> filterempdata",
						"filterempdata sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          custId as string,",
						"          CustName as string,",
						"          itemName as string,",
						"          quantity as string",
						"     ),",
						"     partitionFileNames:['emp_param_mapping.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkemp"
					]
				}
			},
			"dependsOn": []
		}
	]
}