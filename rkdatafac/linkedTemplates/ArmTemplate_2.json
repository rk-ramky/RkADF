{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "rkdatafac"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/df_Conditional Split')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "Emp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkdept10"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkdept20"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkdept30"
						}
					],
					"transformations": [
						{
							"name": "splitondeptno"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as string,",
						"          ename as string,",
						"          job as string,",
						"          mgr as string,",
						"          hiredate as string,",
						"          sal as string,",
						"          comm as string,",
						"          deptno as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Emp",
						"Emp split(equals(deptno, '10'),",
						"     equals(deptno, '20'),",
						"     equals(deptno, '30'),",
						"     disjoint: false) ~> splitondeptno@(dept10, dept20, dept30, otherdepts)",
						"splitondeptno@dept10 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['dept10'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkdept10",
						"splitondeptno@dept20 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['dept20'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkdept20",
						"splitondeptno@dept30 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['dept30'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkdept30"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Exists')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Dept",
								"type": "DatasetReference"
							},
							"name": "sourcdept"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkempdept"
						}
					],
					"transformations": [
						{
							"name": "existsempdept"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as string,",
						"          ename as string,",
						"          job as string,",
						"          mgr as string,",
						"          hiredate as string,",
						"          sal as string,",
						"          comm as string,",
						"          deptno as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceemp",
						"source(output(",
						"          deptno as string,",
						"          dname as string,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcdept",
						"sourceemp, sourcdept exists(sourceemp@deptno == sourcdept@deptno,",
						"     negate:true,",
						"     broadcast: 'auto')~> existsempdept",
						"existsempdept sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['deptexists.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkempdept"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_FlattenTran')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Source_Json",
								"type": "DatasetReference"
							},
							"name": "sourcerk"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkflat"
						}
					],
					"transformations": [
						{
							"name": "flattenrkjson"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as string,",
						"          name as string,",
						"          skills as string[],",
						"          Address as (state as string, country as string, zipcode as string),",
						"          Contact as (Phone as string, email as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'singleDocument') ~> sourcerk",
						"sourcerk foldDown(unroll(skills),",
						"     mapColumn(",
						"          id,",
						"          name,",
						"          skills,",
						"          Address = Address.state,",
						"          {Address.zipcode} = Address.country,",
						"          Contact = Contact.Phone,",
						"          {Contact.Phone} = Contact.email",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flattenrkjson",
						"flattenrkjson sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['rk_json_flatten.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkflat"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Flowlet')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sourceempl"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkflowlet"
						}
					],
					"transformations": [
						{
							"name": "aggregatedup"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Id as integer,",
						"          Name as string,",
						"          department as integer,",
						"          location as string,",
						"          Gender as string,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceempl",
						"sourceempl aggregate(groupBy(Id,",
						"          Name,",
						"          department,",
						"          salary),",
						"     count = count()) ~> aggregatedup",
						"aggregatedup sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['empflowlet.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          Id,",
						"          Name,",
						"          department,",
						"          salary",
						"     ),",
						"     partitionBy('hash', 1)) ~> sinkflowlet"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Flowletdept')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source2",
								"type": "DatasetReference"
							},
							"name": "sourcedep"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_SQL_Blob",
								"type": "DatasetReference"
							},
							"name": "sinkdep"
						}
					],
					"transformations": [
						{
							"name": "aggregatedep"
						}
					],
					"scriptLines": [
						"source(output(",
						"          deptno as integer,",
						"          dname as string,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcedep",
						"sourcedep aggregate(groupBy(deptno,",
						"          dname,",
						"          location),",
						"     count = count()) ~> aggregatedep",
						"aggregatedep sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sinkdep"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Lookup')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Dept",
								"type": "DatasetReference"
							},
							"name": "sourcedept"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinklookup"
						}
					],
					"transformations": [
						{
							"name": "lookupempdept"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as string,",
						"          ename as string,",
						"          job as string,",
						"          mgr as string,",
						"          hiredate as string,",
						"          sal as string,",
						"          comm as string,",
						"          deptno as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceemp",
						"source(output(",
						"          deptno as string,",
						"          dname as string,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcedept",
						"sourceemp, sourcedept lookup(sourceemp@deptno == sourcedept@deptno,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     broadcast: 'auto')~> lookupempdept",
						"lookupempdept sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['empdeptlookup.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinklookup"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_NewBranch')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Dept",
								"type": "DatasetReference"
							},
							"name": "sourcedept"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "aggregateemp"
						},
						{
							"name": "joinempdept"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as integer,",
						"          ename as string,",
						"          job as string,",
						"          mgr as integer,",
						"          hiredate as string,",
						"          sal as integer,",
						"          comm as integer,",
						"          deptno as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceemp",
						"source(output(",
						"          deptno as integer,",
						"          dname as string,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcedept",
						"sourceemp aggregate(groupBy(deptno),",
						"     {Total Employees} = count(empno)) ~> aggregateemp",
						"sourceemp, sourcedept join(sourceemp@deptno == sourcedept@deptno,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinempdept",
						"joinempdept sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['empjoinout.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1",
						"aggregateemp sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['emptotal.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink2"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_ParameterizeMapping')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sourceempnew"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkemp"
						}
					],
					"transformations": [
						{
							"name": "filterempdata"
						}
					],
					"scriptLines": [
						"parameters{",
						"     Department as integer",
						"}",
						"source(output(",
						"          id as integer,",
						"          Name as string,",
						"          Gender as string,",
						"          salary as integer,",
						"          department as integer,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceempnew",
						"sourceempnew filter(department == $Department) ~> filterempdata",
						"filterempdata sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['emp_param_mapping.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkemp"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Parse')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_SQ",
								"type": "DatasetReference"
							},
							"name": "sourcetblemployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkemployees"
						}
					],
					"transformations": [
						{
							"name": "parseskills"
						},
						{
							"name": "parseaddress"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as integer,",
						"          empName as string,",
						"          skills as string,",
						"          address as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> sourcetblemployees",
						"sourcetblemployees parse(Parseskills = skills ? (skill1 as string,",
						"          skill2 as string,",
						"          skill3 as string),",
						"     format: 'delimited',",
						"     columnNamesAsHeader: false,",
						"     columnDelimiter: '|',",
						"     nullValue: '') ~> parseskills",
						"parseskills parse(parsedjson = address ? (city as string,",
						"          country as string),",
						"     format: 'json',",
						"     documentForm: 'singleDocument') ~> parseaddress",
						"parseaddress sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['empparse.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empId,",
						"          empName,",
						"          skill1 = Parseskills.skill1,",
						"          skill2 = Parseskills.skill2,",
						"          skill3 = Parseskills.skill3,",
						"          city = parsedjson.city,",
						"          country = parsedjson.country",
						"     ),",
						"     partitionBy('hash', 1)) ~> sinkemployees"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Pivot')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "sourceEmpNew"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkPivot"
						}
					],
					"transformations": [
						{
							"name": "pivotTrans"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          email as string,",
						"          location as string,",
						"          department as string,",
						"          Gender as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceEmpNew",
						"sourceEmpNew pivot(groupBy(department),",
						"     pivotBy(Gender),",
						"     {} = count(id),",
						"     columnNaming: 'Total_$N$V_Employees',",
						"     lateral: true) ~> pivotTrans",
						"pivotTrans sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['EmpPivot.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkPivot"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Rank')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinksal"
						}
					],
					"transformations": [
						{
							"name": "ranksal"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as integer,",
						"          ename as string,",
						"          job as string,",
						"          mgr as integer,",
						"          hiredate as date,",
						"          sal as integer,",
						"          comm as integer,",
						"          deptno as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     preferredIntegralType: 'integer') ~> sourceemp",
						"sourceemp rank(desc(sal, true),",
						"     output(SalaryRank as long),",
						"     dense: true) ~> ranksal",
						"ranksal sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Emp_Rank.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinksal"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Select')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkselect"
						}
					],
					"transformations": [
						{
							"name": "selectemp"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as integer,",
						"          ename as string,",
						"          job as string,",
						"          mgr as integer,",
						"          hiredate as date,",
						"          sal as integer,",
						"          comm as integer,",
						"          deptno as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceemp",
						"sourceemp select(mapColumn(",
						"          empNumber = empno,",
						"          deptno,",
						"          Ename = ename,",
						"          salary = sal",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectemp",
						"selectemp sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['empselect.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkselect"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Stringfy')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Source_Json",
								"type": "DatasetReference"
							},
							"name": "sourcejkjson"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkjkjson"
						}
					],
					"transformations": [
						{
							"name": "stringifyjkjson"
						},
						{
							"name": "derivedjkjson"
						}
					],
					"scriptLines": [
						"source(output(",
						"          name as string,",
						"          skills as string[],",
						"          contact as (mobile as string, landline as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> sourcejkjson",
						"sourcejkjson stringify(sf_contact = contact ? string,",
						"     format: 'json') ~> stringifyjkjson",
						"stringifyjkjson derive(sf_contact = toString(sf_contact)) ~> derivedjkjson",
						"derivedjkjson sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['jk_stringfy.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          name,",
						"          contact = sf_contact",
						"     ),",
						"     partitionBy('hash', 1)) ~> sinkjkjson"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_UDFunctions')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sourceEmployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumnudf"
						}
					],
					"udfLibraries": [
						{
							"referenceName": "UDFunctions",
							"type": "DataFlowReference"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          department as integer,",
						"          location as string,",
						"          Gender as string,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     preferredIntegralType: 'integer') ~> sourceEmployees",
						"sourceEmployees derive(Gender = GenderValue(Gender)) ~> derivedColumnudf",
						"derivedColumnudf sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['empudf.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Union')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "sourceempdata1"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_Source2",
								"type": "DatasetReference"
							},
							"name": "sourceempdata2"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sourceempdata3"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkempdata"
						}
					],
					"transformations": [
						{
							"name": "unionempdata"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          email as string,",
						"          location as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceempdata1",
						"source(output(",
						"          id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          email as string,",
						"          location as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceempdata2",
						"source(output(",
						"          id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          email as string,",
						"          location as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceempdata3",
						"sourceempdata1, sourceempdata2, sourceempdata3 union(byName: true)~> unionempdata",
						"unionempdata sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['emadatafull.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkempdata"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_Var')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "sourceempinfo"
						}
					],
					"sinks": [
						{
							"name": "sinkmaxsal"
						}
					],
					"transformations": [
						{
							"name": "aggregatesal"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as integer,",
						"          Name as string,",
						"          salary as integer,",
						"          key as integer,",
						"          department as integer,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceempinfo",
						"sourceempinfo aggregate(MaxSalary = max(salary)) ~> aggregatesal",
						"aggregatesal sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1) ~> sinkmaxsal"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_WindowTran')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "sourcewindowtran"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkwindow"
						}
					],
					"transformations": [
						{
							"name": "windowemp"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as integer,",
						"          ename as string,",
						"          job as string,",
						"          mgr as integer,",
						"          hiredate as date,",
						"          sal as integer,",
						"          comm as integer,",
						"          deptno as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcewindowtran",
						"sourcewindowtran window(over(deptno),",
						"     desc(sal, true),",
						"     DenseRank = denseRank()) ~> windowemp",
						"windowemp sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['emp_window.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkwindow"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_assert')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_Source3",
								"type": "DatasetReference"
							},
							"name": "sourceemp"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Dept",
								"type": "DatasetReference"
							},
							"name": "sourcedept"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkempdept"
						}
					],
					"transformations": [
						{
							"name": "assertemp"
						},
						{
							"name": "derivedColumnerror"
						},
						{
							"name": "filterunwanted"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as integer,",
						"          ename as string,",
						"          hiredate as string,",
						"          sal as integer,",
						"          deptno as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourceemp",
						"source(output(",
						"          deptno as integer,",
						"          dname as string,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> sourcedept",
						"sourceemp, sourcedept assert(expectTrue(!isNull(toDate(hiredate, 'yyyyMMdd')), false, 'assertdoj', null, 'checking date of join'),",
						"     expectUnique(empno, false, 'assertempid', null, 'valid emp id'),",
						"     expectExists(sourceemp@deptno == sourcedept@deptno, false, 'assertdeptno', null, 'valid from dept table')) ~> assertemp",
						"assertemp derive(IsErrorRow = isError(),",
						"          IsInCorrectDeptrow = hasError('assertdeptno')) ~> derivedColumnerror",
						"derivedColumnerror filter(IsErrorRow == false()) ~> filterunwanted",
						"filterunwanted sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['empassert.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkempdept"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_derivedcolumns')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV_Emp",
								"type": "DatasetReference"
							},
							"name": "empsource"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_CSV",
								"type": "DatasetReference"
							},
							"name": "sinkderived"
						}
					],
					"transformations": [
						{
							"name": "derivedColumnlocation"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empno as string,",
						"          ename as string,",
						"          job as string,",
						"          mgr as string,",
						"          hiredate as string,",
						"          sal as string,",
						"          comm as string,",
						"          deptno as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> empsource",
						"empsource derive(jobName = initCap(mgr),",
						"          NewHiredate = iif(isNull(hiredate),'Unknown',hiredate)) ~> derivedColumnlocation",
						"derivedColumnlocation sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['empDerived.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sinkderived"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_error_rows')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_Azure_source1",
								"type": "DatasetReference"
							},
							"name": "SalesInd"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_Azure_SQL",
								"type": "DatasetReference"
							},
							"name": "SalesBad"
						},
						{
							"dataset": {
								"referenceName": "DS_Azure_SQ",
								"type": "DatasetReference"
							},
							"name": "SalesGood"
						}
					],
					"transformations": [
						{
							"name": "BadRecs"
						},
						{
							"name": "ForExtraColumn"
						},
						{
							"name": "forExtraColumnGood"
						}
					],
					"scriptLines": [
						"source(output(",
						"          saleDate as string,",
						"          salesitem as string,",
						"          country as string,",
						"          qunatity as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     preferredIntegralType: 'integer') ~> SalesInd",
						"SalesInd split(isNull(toDate(saleDate,'dd-MMM-yyyy')),",
						"     disjoint: false) ~> BadRecs@(ErrorRecs, GoodRecs)",
						"BadRecs@ErrorRecs derive(fileName = 'RK') ~> ForExtraColumn",
						"BadRecs@GoodRecs derive(FileName = 'RK',",
						"          saleDate = toDate(saleDate, 'dd-MMM-yyyy'),",
						"          qunatity = toInteger(qunatity)) ~> forExtraColumnGood",
						"ForExtraColumn sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          saleDate as string,",
						"          salesitem as string,",
						"          country as string,",
						"          qunatity as string,",
						"          FILENAME as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          saleDate,",
						"          salesitem,",
						"          country,",
						"          qunatity,",
						"          FILENAME = fileName",
						"     )) ~> SalesBad",
						"forExtraColumnGood sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          saleDate as date,",
						"          salesitem as string,",
						"          country as string,",
						"          qunatity as integer,",
						"          FILENAME as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          saleDate,",
						"          salesitem,",
						"          country,",
						"          qunatity,",
						"          FILENAME = FileName",
						"     )) ~> SalesGood"
					]
				}
			},
			"dependsOn": []
		}
	]
}